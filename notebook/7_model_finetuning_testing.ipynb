{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0377a2-0b1b-4345-a35a-2774775ce90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/11/2022 14:50:28 - INFO - readmission.modeling_readmission -   loading archive file /PatientTM/model/pretraining/\n",
      "03/11/2022 14:50:28 - INFO - readmission.modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"ccs_vocab_size\": 485,\n",
      "  \"cui_maxlen\": 47,\n",
      "  \"cui_vocab_size\": 19485,\n",
      "  \"daystoprevadmit_hidden_size\": 50,\n",
      "  \"duration_hidden_size\": 50,\n",
      "  \"embedding_hidden_size\": 768,\n",
      "  \"extra_feat_act\": \"mish\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"small_icd9_ccs_maxlen\": 39,\n",
      "  \"small_icd9_vocab_size\": 1335,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/11/2022 14:50:30 - INFO - readmission.modeling_readmission -   Weights of BertForSequenceClassificationRuntimeClinicalText not initialized from pretrained model: ['final_classifier.weight', 'final_classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/PatientTM/src/\")\n",
    "\n",
    "from readmission.modeling_readmission import BertForSequenceClassificationRuntimeClinicalText\n",
    "\n",
    "model_path = \"/PatientTM/model/pretraining/\"\n",
    "features = [\"clinical_text\"]\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassificationRuntimeClinicalText.from_pretrained(model_path, num_labels, features)\n",
    "\n",
    "finetunable_layers = 2\n",
    "freezable_layers = 12 - finetunable_layers\n",
    "NUM_LAYERS_PER_ENCODER = 16\n",
    "params = list(model.named_parameters())  \n",
    "freezable_params = params[:5+freezable_layers*NUM_LAYERS_PER_ENCODER]\n",
    "for name, param in freezable_params:          \n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a168b6f8-a693-4903-9076-a81d6216183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 203 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.gamma                               (768,)\n",
      "bert.embeddings.LayerNorm.beta                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.beta                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "final_classifier.weight                                     (2, 768)\n",
      "final_classifier.bias                                           (2,)\n",
      "\n",
      "==== Extra ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.gamma                               (768,)\n",
      "bert.embeddings.LayerNorm.beta                                (768,)\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.1.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.1.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.1.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.1.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.1.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.1.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.1.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.1.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.1.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.1.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.1.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.1.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.1.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.2.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.2.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.2.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.2.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.2.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.2.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.2.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.2.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.2.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.2.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.2.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.2.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.2.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.3.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.3.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.3.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.3.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.3.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.3.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.3.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.3.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.3.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.3.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.3.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.3.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.3.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.4.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.4.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.4.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.4.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.4.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.4.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.4.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.4.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.4.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.4.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.4.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.4.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.4.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.5.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.5.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.5.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.5.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.5.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.5.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.5.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.5.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.5.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.5.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.5.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.5.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.5.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.6.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.6.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.6.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.6.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.6.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.6.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.6.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.6.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.6.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.6.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.6.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.6.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.6.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.7.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.7.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.7.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.7.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.7.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.7.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.7.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.7.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.7.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.7.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.7.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.7.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.7.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.8.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.8.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.8.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.8.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.8.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.8.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.8.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.8.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.8.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.8.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.8.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.8.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.8.output.LayerNorm.beta                    (768,)\n",
      "bert.encoder.layer.9.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.9.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.9.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.9.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.9.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.9.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.9.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.9.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma         (768,)\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta          (768,)\n",
      "bert.encoder.layer.9.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.9.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.9.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.9.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma                   (768,)\n",
      "bert.encoder.layer.9.output.LayerNorm.beta                    (768,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())        \n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "    \n",
    "finetunable_layers = 2\n",
    "freezable_layers = 12 - finetunable_layers\n",
    "\n",
    "NUM_LAYERS_PER_ENCODER = 16\n",
    "print('\\n==== Extra ====\\n')\n",
    "for p in params[:5+freezable_layers*NUM_LAYERS_PER_ENCODER]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a7e98-6e9a-4757-bd2d-f8118595a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
